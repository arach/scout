{
  "timestamp": "2025-07-10T20:25:40.142566+00:00",
  "test_description": "Real transcription comparison: chunk sizes vs full recording",
  "recordings_analyzed": [
    {
      "recording_name": "UltraShort_recording_3",
      "duration_ms": 1417,
      "category": "UltraShort",
      "full_recording_transcription": "push to record.",
      "full_recording_time_ms": 7980.0,
      "chunk_results": [
        {
          "chunk_size_ms": 1000,
          "transcription": "push to record.",
          "time_to_first_chunk_ms": 1000.0,
          "total_time_ms": 127.0,
          "chunks_processed": 2,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "push to record",
              "chunked_sentence": "push to record",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 2000,
          "transcription": "push to record.",
          "time_to_first_chunk_ms": 2000.0,
          "total_time_ms": 121.0,
          "chunks_processed": 1,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "push to record",
              "chunked_sentence": "push to record",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 3000,
          "transcription": "push to record.",
          "time_to_first_chunk_ms": 3000.0,
          "total_time_ms": 130.0,
          "chunks_processed": 1,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "push to record",
              "chunked_sentence": "push to record",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 5000,
          "transcription": "push to record.",
          "time_to_first_chunk_ms": 5000.0,
          "total_time_ms": 116.0,
          "chunks_processed": 1,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "push to record",
              "chunked_sentence": "push to record",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [],
          "overall_similarity_score": 1.0
        }
      ]
    },
    {
      "recording_name": "Short_recording_6",
      "duration_ms": 3332,
      "category": "Short",
      "full_recording_transcription": "Thanks, let's see how that works.  Long.  you",
      "full_recording_time_ms": 252.0,
      "chunk_results": [
        {
          "chunk_size_ms": 1000,
          "transcription": "Thanks, let's see how that works.  Long.  you",
          "time_to_first_chunk_ms": 1000.0,
          "total_time_ms": 254.0,
          "chunks_processed": 4,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "Thanks, let's see how that works",
              "chunked_sentence": "Thanks, let's see how that works",
              "similarity_score": 1.0
            },
            {
              "sentence_index": 1,
              "full_recording_sentence": "Long",
              "chunked_sentence": "Long",
              "similarity_score": 1.0
            },
            {
              "sentence_index": 2,
              "full_recording_sentence": "you",
              "chunked_sentence": "you",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [
            {
              "chunk_boundary_time_ms": 3000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            }
          ],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 2000,
          "transcription": "Thanks, let's see how that works.  Long.  you",
          "time_to_first_chunk_ms": 2000.0,
          "total_time_ms": 253.0,
          "chunks_processed": 2,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "Thanks, let's see how that works",
              "chunked_sentence": "Thanks, let's see how that works",
              "similarity_score": 1.0
            },
            {
              "sentence_index": 1,
              "full_recording_sentence": "Long",
              "chunked_sentence": "Long",
              "similarity_score": 1.0
            },
            {
              "sentence_index": 2,
              "full_recording_sentence": "you",
              "chunked_sentence": "you",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 3000,
          "transcription": "Thanks, let's see how that works.  Long.  you",
          "time_to_first_chunk_ms": 3000.0,
          "total_time_ms": 248.0,
          "chunks_processed": 2,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "Thanks, let's see how that works",
              "chunked_sentence": "Thanks, let's see how that works",
              "similarity_score": 1.0
            },
            {
              "sentence_index": 1,
              "full_recording_sentence": "Long",
              "chunked_sentence": "Long",
              "similarity_score": 1.0
            },
            {
              "sentence_index": 2,
              "full_recording_sentence": "you",
              "chunked_sentence": "you",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 5000,
          "transcription": "Thanks, let's see how that works.  Long.  you",
          "time_to_first_chunk_ms": 5000.0,
          "total_time_ms": 249.0,
          "chunks_processed": 1,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "Thanks, let's see how that works",
              "chunked_sentence": "Thanks, let's see how that works",
              "similarity_score": 1.0
            },
            {
              "sentence_index": 1,
              "full_recording_sentence": "Long",
              "chunked_sentence": "Long",
              "similarity_score": 1.0
            },
            {
              "sentence_index": 2,
              "full_recording_sentence": "you",
              "chunked_sentence": "you",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [],
          "overall_similarity_score": 1.0
        }
      ]
    },
    {
      "recording_name": "Medium_recording_9",
      "duration_ms": 6881,
      "category": "Medium",
      "full_recording_transcription": "Okay, well our system doesn't seem to want to use profanity anymore.",
      "full_recording_time_ms": 190.0,
      "chunk_results": [
        {
          "chunk_size_ms": 1000,
          "transcription": "Okay, well our system doesn't seem to want to use profanity anymore.",
          "time_to_first_chunk_ms": 1000.0,
          "total_time_ms": 179.0,
          "chunks_processed": 7,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "Okay, well our system doesn't seem to want to use profanity anymore",
              "chunked_sentence": "Okay, well our system doesn't seem to want to use profanity anymore",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [
            {
              "chunk_boundary_time_ms": 3000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 6000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            }
          ],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 2000,
          "transcription": "Okay, well our system doesn't seem to want to use profanity anymore.",
          "time_to_first_chunk_ms": 2000.0,
          "total_time_ms": 178.0,
          "chunks_processed": 4,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "Okay, well our system doesn't seem to want to use profanity anymore",
              "chunked_sentence": "Okay, well our system doesn't seem to want to use profanity anymore",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [
            {
              "chunk_boundary_time_ms": 6000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            }
          ],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 3000,
          "transcription": "Okay, well our system doesn't seem to want to use profanity anymore.",
          "time_to_first_chunk_ms": 3000.0,
          "total_time_ms": 186.0,
          "chunks_processed": 3,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "Okay, well our system doesn't seem to want to use profanity anymore",
              "chunked_sentence": "Okay, well our system doesn't seem to want to use profanity anymore",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 5000,
          "transcription": "Okay, well our system doesn't seem to want to use profanity anymore.",
          "time_to_first_chunk_ms": 5000.0,
          "total_time_ms": 188.0,
          "chunks_processed": 2,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "Okay, well our system doesn't seem to want to use profanity anymore",
              "chunked_sentence": "Okay, well our system doesn't seem to want to use profanity anymore",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [],
          "overall_similarity_score": 1.0
        }
      ]
    },
    {
      "recording_name": "Long_recording_12",
      "duration_ms": 42553,
      "category": "Long",
      "full_recording_transcription": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
      "full_recording_time_ms": 853.0,
      "chunk_results": [
        {
          "chunk_size_ms": 1000,
          "transcription": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
          "time_to_first_chunk_ms": 1000.0,
          "total_time_ms": 848.0,
          "chunks_processed": 43,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
              "chunked_sentence": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [
            {
              "chunk_boundary_time_ms": 3000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 6000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 9000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 12000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 15000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 18000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 21000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 24000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 27000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 30000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 33000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 36000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 39000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 42000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            }
          ],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 2000,
          "transcription": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
          "time_to_first_chunk_ms": 2000.0,
          "total_time_ms": 969.0,
          "chunks_processed": 22,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
              "chunked_sentence": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [
            {
              "chunk_boundary_time_ms": 6000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 12000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 18000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 24000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 30000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 36000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 42000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            }
          ],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 3000,
          "transcription": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
          "time_to_first_chunk_ms": 3000.0,
          "total_time_ms": 866.0,
          "chunks_processed": 15,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
              "chunked_sentence": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [
            {
              "chunk_boundary_time_ms": 9000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 18000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 27000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 36000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            }
          ],
          "overall_similarity_score": 1.0
        },
        {
          "chunk_size_ms": 5000,
          "transcription": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
          "time_to_first_chunk_ms": 5000.0,
          "total_time_ms": 869.0,
          "chunks_processed": 9,
          "word_differences": [],
          "sentence_differences": [
            {
              "sentence_index": 0,
              "full_recording_sentence": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
              "chunked_sentence": "In addition to that, I'm thinking that there might be actual value in  Potentially, I'm not saying this for a fact, but potentially having progressive  improvements on transcription meaning that we could have like a quick first snap at the transcription and  Then a behind-the-scenes process of improving the transcription with a more  Sophisticated model where you would do a diff and then kind of come up with a  Determination of whether or not the diff is improving or not which probably will right  But but that's kind of something that I've been thinking about",
              "similarity_score": 1.0
            }
          ],
          "boundary_artifacts": [
            {
              "chunk_boundary_time_ms": 15000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            },
            {
              "chunk_boundary_time_ms": 30000.0,
              "issue_description": "Word fragmentation at chunk boundary",
              "impact_on_transcription": "Potential missing or repeated words"
            }
          ],
          "overall_similarity_score": 1.0
        }
      ]
    }
  ],
  "summary": {
    "total_recordings": 4,
    "chunk_sizes_tested": [
      1000,
      2000,
      3000,
      5000
    ],
    "key_findings": [
      "UltraShort (1417ms): Best=5000ms chunks (1.000 similarity), Worst=1000ms chunks (1.000 similarity)",
      "Short (3332ms): Best=5000ms chunks (1.000 similarity), Worst=1000ms chunks (1.000 similarity)",
      "Medium (6881ms): Best=5000ms chunks (1.000 similarity), Worst=1000ms chunks (1.000 similarity)",
      "Long (42553ms): Best=5000ms chunks (1.000 similarity), Worst=1000ms chunks (1.000 similarity)"
    ],
    "quality_degradation_patterns": [
      "1000ms chunks in Short recording: 1 boundary artifacts detected",
      "1000ms chunks in Medium recording: 2 boundary artifacts detected",
      "2000ms chunks in Medium recording: 1 boundary artifacts detected",
      "1000ms chunks in Long recording: 14 boundary artifacts detected",
      "2000ms chunks in Long recording: 7 boundary artifacts detected",
      "3000ms chunks in Long recording: 4 boundary artifacts detected",
      "5000ms chunks in Long recording: 2 boundary artifacts detected"
    ],
    "recommendations": [
      "Medium recordings: Optimal chunk size 5000ms (similarity: 1.000)",
      "UltraShort recordings: Optimal chunk size 5000ms (similarity: 1.000)",
      "Short recordings: Optimal chunk size 5000ms (similarity: 1.000)",
      "Long recordings: Optimal chunk size 5000ms (similarity: 1.000)"
    ]
  }
}