use std::path::PathBuf;
use std::time::Instant;
use tokio;
use std::sync::Arc;
use tokio::sync::Mutex;

// Import Scout components
use scout_lib::transcription::Transcriber;

// This test demonstrates that the singleton transcriber eliminates Core ML recompilation overhead
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("üß™ Testing Singleton Transcriber Performance");
    println!("===========================================\n");
    
    let model_path = PathBuf::from("./models/ggml-base.en.bin");
    if !model_path.exists() {
        println!("‚ùå Model file not found at: {:?}", model_path);
        println!("Please ensure the Whisper model is downloaded.");
        return Ok(());
    }
    
    // Simulate the singleton pattern used in AppState
    let transcriber_singleton = Arc::new(Mutex::new(None::<Transcriber>));
    let current_model_path = Arc::new(Mutex::new(None::<PathBuf>));
    
    // Function to get or create transcriber (mimics AppState behavior)
    async fn get_transcriber(
        model_path: &PathBuf, 
        transcriber_ref: Arc<Mutex<Option<Transcriber>>>,
        current_model_ref: Arc<Mutex<Option<PathBuf>>>
    ) -> Result<(), String> {
        let mut current_model = current_model_ref.lock().await;
        let mut transcriber_opt = transcriber_ref.lock().await;
        
        // Check if we need to create a new transcriber
        let needs_new_transcriber = match (&*current_model, &*transcriber_opt) {
            (Some(current_path), Some(_)) if current_path == model_path => false,
            _ => true
        };
        
        if needs_new_transcriber {
            println!("üîÑ Creating new singleton transcriber...");
            let start = Instant::now();
            
            match Transcriber::new(model_path) {
                Ok(new_transcriber) => {
                    let creation_time = start.elapsed();
                    println!("‚úÖ Transcriber created in: {:.2}s", creation_time.as_secs_f64());
                    *transcriber_opt = Some(new_transcriber);
                    *current_model = Some(model_path.clone());
                }
                Err(e) => return Err(e)
            }
        } else {
            println!("‚ôªÔ∏è  Reusing existing singleton transcriber (no recompilation)");
        }
        
        Ok(())
    }
    
    // Test audio file path - using any existing test file or create a dummy one
    let test_audio = PathBuf::from("./test_audio.wav");
    if !test_audio.exists() {
        println!("üìÅ No test audio file found, creating dummy test");
        println!("   (In real usage, you'd have actual audio files)");
    }
    
    println!("üöÄ Testing singleton behavior:\n");
    
    // First call - should create transcriber (with potential Core ML compilation)
    println!("1Ô∏è‚É£ First transcription call:");
    let start = Instant::now();
    get_transcriber(&model_path, transcriber_singleton.clone(), current_model_path.clone()).await?;
    let first_call_time = start.elapsed();
    println!("   Time for first call: {:.2}s\n", first_call_time.as_secs_f64());
    
    // Second call - should reuse existing transcriber (no Core ML recompilation)
    println!("2Ô∏è‚É£ Second transcription call:");
    let start = Instant::now();
    get_transcriber(&model_path, transcriber_singleton.clone(), current_model_path.clone()).await?;
    let second_call_time = start.elapsed();
    println!("   Time for second call: {:.2}s\n", second_call_time.as_secs_f64());
    
    // Third call - should also reuse (confirming persistence)
    println!("3Ô∏è‚É£ Third transcription call:");
    let start = Instant::now();
    get_transcriber(&model_path, transcriber_singleton.clone(), current_model_path.clone()).await?;
    let third_call_time = start.elapsed();
    println!("   Time for third call: {:.2}s\n", third_call_time.as_secs_f64());
    
    // Results analysis
    println!("üìä Performance Analysis:");
    println!("========================");
    println!("First call (with model loading):  {:.2}s", first_call_time.as_secs_f64());
    println!("Second call (singleton reuse):    {:.2}s", second_call_time.as_secs_f64());
    println!("Third call (singleton reuse):     {:.2}s", third_call_time.as_secs_f64());
    
    if second_call_time.as_millis() < 100 && third_call_time.as_millis() < 100 {
        println!("\n‚úÖ SUCCESS: Singleton is working!");
        println!("   - Subsequent calls are <100ms (no recompilation)");
        println!("   - Core ML model is being reused properly");
    } else {
        println!("\n‚ö†Ô∏è  WARNING: Singleton may not be working optimally");
        println!("   - Check if Core ML model is being recompiled on each call");
    }
    
    let speedup = first_call_time.as_secs_f64() / second_call_time.as_secs_f64();
    if speedup > 10.0 {
        println!("   - Speedup: {:.1}x faster after first load", speedup);
    }
    
    println!("\nüéØ This demonstrates the singleton pattern eliminates Core ML recompilation overhead!");
    
    Ok(())
}